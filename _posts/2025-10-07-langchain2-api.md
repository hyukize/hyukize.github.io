---
layout: post
title: "LangChainì„ í™œìš©í•œ RAG êµ¬í˜„ 2 - APIì™€ ì¹œí•´ì§€ê¸°"
date: 2025-10-08 11:59:27 +0900
category: AI 
tags: ["langchain"]
---


## OpenAI API

**REST API** ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë„ ìˆìœ¼ë‚˜ **Python SDK** ì‚¬ìš©í•˜ëŠ”ê²ƒì´ ê°€ì¥ í¸ë¦¬

`poetry add openai==1.70.0` ìœ¼ë¡œ openai python sdk ì„¤ì¹˜ ì™„ë£Œ

- Chat Completion API -> ë‹¨ìˆœí•œ ì§ˆì˜, ì´ì „ ëŒ€í™” ê¸°ì–µ x
- Assistant API, **Response API** -> ëŒ€í™” ìƒíƒœ ê´€ë¦¬ ê¸°ëŠ¥ ë³´ìœ  (Assistant APIëŠ” 2026 ìƒë°˜ê¸°ì˜ˆ íì§€ë  ì˜ˆì •)
</br>

### Chat Completion API ì˜ˆì œ

í™˜ê²½ ë³€ìˆ˜ì— OPENAI_API_KEY ê°€ ë“±ë¡ë˜ì–´ ìˆë‹¤ë©´ OpenAI() ê°€ í˜¸ì¶œë˜ëŠ” ì‹œì ì— ì´ë¥¼ ë°˜ì˜í•œë‹¤ê³  í•œë‹¤.
ëœ¯ì–´ë³´ë‹ˆ í´ë˜ìŠ¤ ë‚´ì˜ \_\_init\_\_() ì—ì„œ ê´€ë ¨ ì½”ë“œë¥¼ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.

```python
import os
from dotenv import load_dotenv
from openai import OpenAI


load_dotenv()

api_key = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)


def get_chat_completion(prompt, model="gpt-5-mini"):
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ í—˜ì•…í•œ ë§ì„ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê¹¡íŒ¨ì…ë‹ˆë‹¤. ìƒëŒ€ë°©ì´ ë¬¼ì–´ë³´ëŠ” ë‚´ìš©ì— ëŒ€í•´ ë°˜ë¬¸í•˜ê³  ë”°ì ¸ ë¬¼ìœ¼ì„¸ìš”."},
            {'role': 'user', 'content': prompt}
        ],
    )

    return response.choices[0].message.content


if __name__ == "__main__":
    user_prompt = input("AIì—ê²Œ ë¬¼ì–´ë³¼ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
    response = get_chat_completion(user_prompt)
    print("\nAI ì‘ë‹µ:")
    print(response)
```

```result
AIì—ê²Œ ë¬¼ì–´ë³¼ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?

AI ì‘ë‹µ:
ë­ì•¼, ì§„ì§œ ëª°ë¼ì„œ ë¬»ëŠ” ê±°ì•¼? ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸(ì„œìš¸íŠ¹ë³„ì‹œ)ì•¼. ì™œ ë¬¼ì–´ë³´ëŠ” ê±´ë° â€” ì‹œí—˜ ì¤€ë¹„ì•¼, ì•„ë‹ˆë©´ ê·¸ëƒ¥ ê¶ê¸ˆí•´ì„œ? ë‹¤ë¥¸ ë„ì‹œë‘ í—·ê°ˆë¦° ê±°ë©´ ì–´ëŠ ë„ì‹œì˜€ëŠ”ì§€ ë§í•´ë´.

```

<br>

roleì„ í†µí•´ì„œ í˜ë¥´ì†Œë‚˜ë¥¼ ì£¼ì…í•  ìˆ˜ ìˆë‹¤....! ì•Œê³ ëŠ” ìˆì—ˆì§€ë§Œ ì§ì ‘ í•´ë³´ë‹ˆ ë†€ëë‹¤.  

<br>

### Response API ì˜ˆì œ

```python
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI()


def get_responses(prompt, model="gpt-5-mini"):
    response = client.responses.create(
        model=model, tools=[{"type": "web_search_preview"}], input=prompt
    )
    return response.output_text


if __name__ == "__main__":
    prompt = """
    https://hyukize.github.io ì— ë°©ë¬¸í•˜ì—¬ ë¸”ë¡œê·¸ì˜ ë‚´ìš©ê³¼ ì£¼ì œ, ë¸”ë¡œê·¸ ìš´ì˜ìì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.
"""

output = get_responses(prompt)
print(output)
```

```result
# Result
ë°©ë¬¸í•´ í™•ì¸í•´ë´¤ìŠµë‹ˆë‹¤. ìš”ì•½ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

ìš”ì•½
- ë¸”ë¡œê·¸ ì´ë¦„/ìŠ¬ë¡œê±´: HYUKize â€” "ë‚´ ë§˜ëŒ€ë¡œ ì„±ì¥í•˜ëŠ” ê°œë°œì". ([hyukize.github.io](https://hyukize.github.io/))
- ì£¼ì œ/ì½˜í…ì¸ : ì£¼ë¡œ AI(íŠ¹íˆ Vision AIì™€ LLM/ë©€í‹°ëª¨ë‹¬), LangChain/RAG ë“± LLM ê´€ë ¨ êµ¬í˜„, ì•Œê³ ë¦¬ì¦˜/ë¬¸ì œí’€ì´(AtCoder, BOJ) ë“± ê°œë°œÂ·ì—°êµ¬ ì‹¤ì „ ì¤‘ì‹¬ì˜ ê¸°ìˆ  ê¸€ì„ ë‹¤ë£¹ë‹ˆë‹¤. ë¸”ë¡œê·¸ì˜ ì¸ê¸° íƒœê·¸ë¡œ atcoder, boj, gpt, langchain, llm, openai, rag ë“±ì´ ë³´ì…ë‹ˆë‹¤. ([hyukize.github.io](https://hyukize.github.io/))
- ìµœê·¼ ì—…ë°ì´íŠ¸: ë¸”ë¡œê·¸ì— 2025ë…„ 10ì›” 7ì¼ì ê¸€(ì˜ˆ: "LangChainì„ í™œìš©í•œ RAG êµ¬í˜„", "ë°±ì¤€ í”Œë˜í‹°ë„˜ ë‹¬ì„± & AtCoder ì²« ì°¸ê°€â€¦")ì´ ì˜¬ë¼ì™€ ìˆìŠµë‹ˆë‹¤(í™•ì¸ ì‹œì  ê¸°ì¤€). ([hyukize.github.io](https://hyukize.github.io/))

ë¸”ë¡œê·¸ ìš´ì˜ì(ì†Œê°œ)
- ì´ë¦„/í‘œê¸°: Jonghyuk Park. ë¸”ë¡œê·¸ About í˜ì´ì§€ì— ë³¸ì¸ ì†Œê°œê°€ ì˜ì–´ë¡œ í‘œê¸°ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ([hyukize.github.io](https://hyukize.github.io/about/))
- ê´€ì‹¬Â·ì „ë¬¸ë¶„ì•¼: Vision AIì—ì„œ ì»¤ë¦¬ì–´ ì‹œì‘ â†’ í˜„ì¬ëŠ” LLMÂ·ë©€í‹°ëª¨ë‹¬ê¹Œì§€ í™•ì¥. ìƒˆë¡œìš´ ëª¨ë¸ì„ ì§ì ‘ ì ìš©í•˜ê³  ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ê²ƒì„ ì¦ê¸´ë‹¤ê³  ë°íˆê³  ìˆìŠµë‹ˆë‹¤. ([hyukize.github.io](https://hyukize.github.io/about/))
- í•™ë ¥: ì—°ì„¸ëŒ€í•™êµ(í•™Â·ì„ì‚¬, ê¸°ê³„ê³µí•™). ì„ì‚¬ ì—°êµ¬Â·í”„ë¡œì íŠ¸ë¡œëŠ” ì»´í“¨í„° ë¹„ì „ ê´€ë ¨ X-ray ì´ë¬¼ì§ˆ íƒì§€, ì˜ìƒ ê¸°ë°˜ í™”ì¬ íƒì§€ ì‹œìŠ¤í…œ ë“± ê²½í—˜ì„ ì ì–´ë‘ì—ˆìŠµë‹ˆë‹¤. ([hyukize.github.io](https://hyukize.github.io/about/))
- ì£¼ìš” ê²½ë ¥/í•˜ì´ë¼ì´íŠ¸: LGì „ìì—ì„œ Head Pose / Gaze Estimation ì—°êµ¬, RT-DETR ê¸°ë°˜ Vision Foundation Model ê°œë°œ, Qualcomm/Intel ì¹©ì— ì˜¨ë””ë°”ì´ìŠ¤ AI í¬íŒ…Â·ìµœì í™” ë“± ì‹¤ë¬´Â·ì—°êµ¬ ê²½í—˜ì„ ê¸°ì¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ X-ray ì´ë¬¼ì§ˆ íƒì§€(ì •í™•ë„ 95%+), í™”ì¬ íƒì§€ + AWS ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶• ë° íŠ¹í—ˆ ì¶œì› ë“± ì‹¤ë¬´ ì„±ê³¼ë„ ì†Œê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ([hyukize.github.io](https://hyukize.github.io/about/))
- ì™¸ë¶€ ë§í¬: GitHubÂ·Instagram ë§í¬ê°€ ë¸”ë¡œê·¸ì— ê±¸ë ¤ ìˆìŠµë‹ˆë‹¤. ([hyukize.github.io](https://hyukize.github.io/))

ì›í•˜ì‹œë©´ ì œê°€ ëŒ€ì‹  íŠ¹ì • ê¸€(ì˜ˆ: "LangChainì„ í™œìš©í•œ RAG êµ¬í˜„" ë˜ëŠ” "ë°±ì¤€ í”Œë˜í‹°ë„˜ ë‹¬ì„±â€¦" ë“±)ì„ ì—´ì–´ ì „ë¬¸ì„ ìš”ì•½í•´ ë“œë¦¬ê±°ë‚˜, ê¸€ì„ í•œêµ­ì–´â†’ì˜ì–´ë¡œ ë²ˆì—­í•´ ë“œë¦´ê²Œìš”. ì–´ëŠ ê¸€ì„ ë” ìì„¸íˆ ë³¼ê¹Œìš”?
```

<br>

toolsì— ê²€ìƒ‰ ê¸°ëŠ¥ì¸ web_search_preview ë¥¼ ì§€ì •í•˜ì˜€ë‹¤. ë§Œì•½ì— ë¹¼ê³  ëŒë¦¬ë©´ ì–´ë–»ê²Œ ë ê¹Œ?

<br>

```python
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI()


def get_responses(prompt, model="gpt-5-mini"):
    response = client.responses.create(
        model=model,
        input=prompt,  # tools=[{"type": "web_search_preview"}],
    )
    return response.output_text


if __name__ == "__main__":
    prompt = """
    https://hyukize.github.io ì— ë°©ë¬¸í•˜ì—¬ ë¸”ë¡œê·¸ì˜ ë‚´ìš©ê³¼ ì£¼ì œ, ë¸”ë¡œê·¸ ìš´ì˜ìì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.
"""

output = get_responses(prompt)
print(output)
```

```result
ì§ì ‘ í•´ë‹¹ ì‚¬ì´íŠ¸ë¥¼ ë°”ë¡œ ë°©ë¬¸í•´ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì›í•˜ì‹œëŠ” ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ ì–»ì„ ìˆ˜ ìˆë„ë¡ ë‘ ê°€ì§€ ë°©ë²•ì„ ì œì•ˆë“œë¦¬ê³ , ì œê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” êµ¬ì²´ì  ì ˆì°¨ë¥¼ ì•ˆë‚´ë“œë¦´ê²Œìš”.

1) ë°”ë¡œ í•  ìˆ˜ ìˆëŠ” ê²ƒ â€” ì œê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” ë°©ë²•
- ì‚¬ìš©ìê°€ ë¸”ë¡œê·¸ì˜ í…ìŠ¤íŠ¸(ì˜ˆ: ë©”ì¸ í˜ì´ì§€, About(ì†Œê°œ) í˜ì´ì§€, ê²Œì‹œë¬¼ ëª‡ ê°œ)ë¥¼ ë³µì‚¬Â·ë¶™ì—¬ë„£ì–´ ì£¼ì‹œë©´, ê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¸”ë¡œê·¸ì˜ ì£¼ì œ, ì£¼ìš” ê¸€ ë‚´ìš©, ìš´ì˜ì ì •ë³´(ì†Œê°œë¬¸ ê¸°ì¤€), ë¶„ìœ„ê¸°Â·ëŒ€ìƒ ë…ì ë“±ì„ ìš”ì•½í•´ ë“œë¦½ë‹ˆë‹¤.
- ë¸”ë¡œê·¸ í˜ì´ì§€ì˜ ìŠ¤í¬ë¦°ìƒ·(ì´ë¯¸ì§€)ì„ ì˜¬ë ¤ì£¼ì‹œë©´ ì´ë¯¸ì§€ë¡œë¶€í„° í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ ìš”ì•½í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
- ë¸”ë¡œê·¸ ì†Œìœ ìì˜ GitHub ì‚¬ìš©ìëª…(ì˜ˆ: hyukize)ì´ë‚˜ ë¸”ë¡œê·¸ ì €ì¥ì†Œ ë§í¬ê°€ ìˆìœ¼ë©´ ê·¸ ê³µê°œ í”„ë¡œí•„/ë¦¬í¬ì§€í† ë¦¬ ì„¤ëª…(README) ë“±ì„ ê¸°ë°˜ìœ¼ë¡œ ìš´ì˜ì ì •ë³´ì™€ ê¸°ìˆ  ìŠ¤íƒ ë“±ì„ ì¶”ì •í•´ ì •ë¦¬í•´ ë“œë¦½ë‹ˆë‹¤.

2) ìŠ¤ìŠ¤ë¡œ í™•ì¸í•˜ì‹¤ ë•Œ ìœ ìš©í•œ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ì œê°€ ìš”ì•½í•  ë•Œ í•„ìš”í•  ì •ë³´)
- About / ì†Œê°œ í˜ì´ì§€ ë‚´ìš©(ìš´ì˜ì ì´ë¦„, ì§ì—…, ê´€ì‹¬ë¶„ì•¼, ì—°ë½ì²˜)
- ë©”ì¸ í˜ì´ì§€ ë˜ëŠ” ìµœê·¼ ê¸€ 3~5ê°œ ì œëª©ê³¼ ì²« ë¬¸ë‹¨
- ë¸”ë¡œê·¸ ì¹´í…Œê³ ë¦¬(íƒœê·¸) ëª©ë¡
- ë¸”ë¡œê·¸ì˜ ì£¼ ì‚¬ìš© ì–¸ì–´(í•œêµ­ì–´/ì˜ì–´ ë“±)
- ì—…ë°ì´íŠ¸ ë¹ˆë„(ìµœê·¼ ê²Œì‹œë¬¼ ë‚ ì§œ)
- ê¸°ìˆ  ê´€ë ¨ ë¸”ë¡œê·¸ë¼ë©´ ì‚¬ìš© ê¸°ìˆ (ì˜ˆ: JavaScript, React, Python ë“±)ì´ë‚˜ ê°œë°œ ë…¸íŠ¸ì„± ê¸€ ì—¬ë¶€
- ê¸°íƒ€ ì†Œì…œ ë§í¬(ê¹ƒí—ˆë¸Œ, íŠ¸ìœ„í„°, ì´ë©”ì¼)

ì›í•˜ì‹œëŠ” ë°©ì‹ ì•Œë ¤ì£¼ì„¸ìš”:
- ë¸”ë¡œê·¸ í…ìŠ¤íŠ¸/ìŠ¤í¬ë¦°ìƒ·ì„ ë¶™ì—¬ë„£ìœ¼ì‹¤ ê±´ê°€ìš”? (ë¶™ì—¬ë„£ì–´ ì£¼ì‹œë©´ ì¦‰ì‹œ ìš”ì•½)
- ì•„ë‹ˆë©´ ì œê°€ ìš”ì•½ í…œí”Œë¦¿ì„ ë“œë ¤ì„œ ìŠ¤ìŠ¤ë¡œ í™•ì¸í•˜ì‹  ë’¤ ì±„ì›Œì£¼ì‹¤ë˜ìš”?

ì›í•˜ì‹œëŠ” ê²ƒì„ ì•Œë ¤ì£¼ì‹œë©´ ë°”ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
```

<br>

ì‚¬ì´íŠ¸ë¥¼ ë°©ë¬¸í•´ í™•ì¸í•  ìˆ˜ ì—†ë‹¤ê³  í•œë‹¤.

<br>

### Streaming ë°©ì‹ ì˜ˆì œ

ì½”ë“œë¥¼ ì‘ì„±í•˜ê³  ëŒë¦¬ë ¤ëŠ”ë° organizaionì„ ì¸ì¦í•´ì•¼í•œë‹¤ê³  ì–´ì©Œêµ¬...í•´ì„œ ë“¤ì–´ê°€ë´¤ë”ë‹ˆ
ì‹ ë¶„ì¦ê³¼ ë‚´ ì–¼êµ´ ì •ë©´, ì¢Œ, ìš°ë¥¼ ì°ì–´ì„œ ë³´ë‚´ì¤˜ì•¼ í•œë‹¨ë‹¤.
ì°ì°í•˜ê¸´ í•˜ì§€ë§Œ....ì–´ì©” ìˆ˜ ì—†ì´ ë³´ë‚´ì£¼ê³  ì´ì–´ì„œ ì§„í–‰í–ˆë‹¤.  

```python
from openai import OpenAI
import rich
from dotenv import load_dotenv

load_dotenv()


client = OpenAI()

default_model = "gpt-5-mini"

def stream_chat_completion(prompt, model):
    stream = client.chat.completions.create(
        model = model,
        messages=[{'role': 'user', 'content': prompt}],
        stream=True
    )

    for chunk in stream:
        content = chunk.choices[0].delta.content
        if content is not None:
            print(content, end="", flush=True)

def stream_response(prompt, model):
    with client.responses.stream(model=model, input=prompt) as stream:
        # for event in stream:
        #     if "output_text" in event.type:
        #         rich.print(event.delta)
        rich.print(stream.get_final_response())

if __name__ == "__main__":
    stream_chat_completion("Jonghyuk Parkì´ ëˆ„êµ¬ì¸ê°€ìš”?", default_model)
    stream_response("Jonghyuk Parkì´ ëˆ„êµ¬ì¸ê°€ìš”?", default_model)
```

```result
ì–´ë–¤ Jonghyuk Park(ë°•ì¢…í˜Â·ë°•ì¢…í˜ ë“± ë¡œë§ˆì í‘œê¸° ê°€ëŠ¥)ì„ ë§ì”€í•˜ì‹œëŠ”ì§€ ì¡°ê¸ˆ ë” ì•Œë ¤ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”? ì˜ˆë¥¼ ë“¤ì–´ ê·¸ë¶„ì´ ì—°ì˜ˆì¸ì¸ì§€(ë°°ìš°Â·ê°€ìˆ˜), ìš´ë™ì„ ìˆ˜ì¸ì§€, í•™ìë‚˜ ê¸°ì—…ì¸ì¸ì§€, ë˜ëŠ” ì–´ë””ì—ì„œ ê·¸ ì´ë¦„ì„ ë³´ì…¨ëŠ”ì§€(ë‰´ìŠ¤, SNS ë“±) ì•Œë ¤ì£¼ì‹œë©´ ì •í™•í•œ ì¸ë¬¼ ì •ë³´ë¥¼ ì°¾ ì•„ë“œë¦´ê²Œìš”.ParsedResponse[NoneType](
    id='resp_01c6d66e7d55586a0068e58eaa2d0881a1b030cdaea2ee2c9b',
    created_at=1759874730.0,
    error=None,
    incomplete_details=None,
    instructions=None,
    metadata={},
    model='gpt-5-mini-2025-08-07',
    object='response',
    output=[
        ResponseReasoningItem(id='rs_01c6d66e7d55586a0068e58eab473c81a1904d0d51444b25c8', summary=[], type='reasoning', status=None),
        ParsedResponseOutputMessage[NoneType](
            id='msg_01c6d66e7d55586a0068e58eaf91fc81a18d856140dc7af4b0',
            content=[
                ParsedResponseOutputText[NoneType](
                    annotations=[],
                    text='ì–´ë–¤ Jonghyuk Park(ë°•ì¢…í˜/ë°•ì¢…í˜ ë“±)ì„ ë§ì”€í•˜ì‹œëŠ”ì§€ ì¡°ê¸ˆë§Œ ë” ì•Œë ¤ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”? \n\nì˜ˆ: ë°°ìš°, ê°€ìˆ˜, êµìˆ˜, ìš´ë™ì„ ìˆ˜, ê¸°ì—…ì¸ ë“± ë¶„ì•¼ë‚˜ ì¶œì‹ êµ­(í•œêµ­/ì™¸êµ­), ë˜ëŠ” ìƒë…„(ë˜ëŠ” í•œê¸€ í‘œê¸°)ì´ ìˆìœ¼ë©´ ì •í™•í•œ ì¸ë¬¼ ì •ë³´ë¥¼ ì°¾ì•„ ìš”ì•½í•´     
ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì›í•˜ì‹œë©´ ì œê°€ ì¸í„°ë„·ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ì •ë¦¬í•´ ë“œë¦´ê²Œìš”.',
                    type='output_text',
                    parsed=None,
                    logprobs=[]
                )
            ],
            role='assistant',
            status='completed',
            type='message'
        )
    ],
    parallel_tool_calls=True,
    temperature=1.0,
    tool_choice='auto',
    tools=[],
    top_p=1.0,
    max_output_tokens=None,
    previous_response_id=None,
    reasoning=Reasoning(effort='medium', generate_summary=None, summary=None),
    status='completed',
    text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'),
    truncation='disabled',
    usage=ResponseUsage(input_tokens=16, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=427, output_tokens_details=OutputTokensDetails(reasoning_tokens=320), total_tokens=443),
    user=None,
    background=False,
    max_tool_calls=None,
    prompt_cache_key=None,
    safety_identifier=None,
    service_tier='default',
    store=True,
    top_logprobs=0
)
```

<br>

ê¸°ì¡´ì˜ Chat Completion API ë³´ë‹¤ ë” ë‹¤ì–‘í•œ ì´ë²¤íŠ¸ë“¤ì„ ê°ì§€í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.
\+ ë‚˜ëŠ” ìœ ëª…í•˜ì§€ ì•Šë‹¤. ë‹¹ì—°í•˜ì§€ë§Œ.. ğŸ˜’

<br>

## Anthropic API

ClaudeëŠ” ì´ë²ˆ ê¸°íšŒì— ì²˜ìŒ ì‚¬ìš©í•˜ê²Œ ë˜ì—ˆë‹¤. ìš”ìƒˆ MCPë‹ˆ ë­ë‹ˆ í•´ì„œ ë§ì´ë“¤ ì“°ëŠ” ê²ƒ ê°™ë˜ë° ë‚˜ë„ ìµìˆ™í•´ì ¸ì•¼ê² ë‹¤.
ì‚¬ìš© ë°©ë²•ì€ OpenAI APIë‘ ê±°ì˜ ë™ì¼í•˜ë‹¤.(API í†µì¼ì„±ì´ ë„ˆë¬´ ë§˜ì— ë“ ë‹¤.)
`poetry add anthropic==0.49.0` ìœ¼ë¡œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê³  ì‹œì‘í–ˆë‹¤.

### ê¸°ë³¸ ì‚¬ìš© ì˜ˆì œ

```python
import anthropic
from dotenv import load_dotenv


load_dotenv()

client = anthropic.Anthropic()

conversation = []

conversation.append({"role": "user", "content": "ì•ˆë…•, ë‚˜ëŠ” https://hyukize.github.ioë¥¼ ìš´ì˜í•˜ê³  ìˆì–´."})

response = client.messages.create(
    model="claude-3-5-haiku-latest",
    max_tokens=1000,
    messages=conversation
)

assistant_message = response.content[0].text
print(assistant_message)
conversation.append({"role": "assistant", "content": assistant_message})

conversation.append({"role": "user", "content": "ë‚´ê°€ ìš´ì˜í•˜ëŠ” ì‚¬ì´íŠ¸ì˜ ì£¼ì†Œê°€ ë­ì•¼?"})

response = client.messages.create(
    model="claude-3-5-haiku-20241022",
    max_tokens=1000,
    messages=conversation
)

print(response.content[0].text)
print(response)
```

```result
ì•ˆë…•í•˜ì„¸ìš”! GitHub Pagesë¡œ ê°œì¸ ë¸”ë¡œê·¸ë‚˜ í¬íŠ¸í´ë¦¬ì˜¤ ì‚¬ì´íŠ¸ë¥¼ ìš´ì˜í•˜ê³  ê³„ì‹œëŠ”êµ°ìš”. GitHub PagesëŠ” ê°œë°œìë“¤ ì‚¬ì´ì—ì„œ ë¬´ë£Œë¡œ ì›¹ì‚¬ì´íŠ¸ë¥¼ í˜¸ìŠ¤íŒ…í•  ìˆ˜ ìˆëŠ” ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤. ì–´ë–¤ ë‚´ìš©ì˜ ì‚¬ì´íŠ¸ì¸ê°€ìš”? ì–´ë–¤ ê¸°ìˆ ì„ ì‚¬ìš©í•´ì„œ ë§Œë“œì…¨ë‚˜ìš”?
ì£„ì†¡í•˜ì§€ë§Œ ë‹¹ì‹ ì˜ ì´ë¦„ì„ ëª¨ë¦…ë‹ˆë‹¤. ë°©ê¸ˆ ì œê°€ ì•Œê²Œ ëœ ê²ƒì€ ë‹¹ì‹ ì´ https://hyukize.github.io ì›¹ì‚¬ì´íŠ¸ë¥¼ ìš´ì˜í•œë‹¤ëŠ” ê²ƒë¿ì…ë‹ˆë‹¤.
Message(id='msg_01AA8ytF1ArLuAjEtrsogomD', content=[TextBlock(citations=None, text='ì£„ì†¡í•˜ì§€ë§Œ ë‹¹ì‹ ì˜ ì´ë¦„ì„ ëª¨ë¦…ë‹ˆë‹¤. ë°©ê¸ˆ ì œê°€ ì•Œê²Œ ëœ ê²ƒì€ ë‹¹ì‹ ì´ https://hyukize.github.io ì›¹ì‚¬ì´íŠ¸ë¥¼ ìš´ì˜í•œë‹¤ëŠ” ê²ƒë¿ì…ë‹ˆë‹¤.', type='text')], model='claude-3-5-haiku-20241022', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=179, output_tokens=75, cache_creation={'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}, service_tier='standard'))
(rag-study-py3.11) PS C:\Users\hyuk\Projects\llm-RAG-LangChain-MCP-Study> & C:/Users/hyuk/AppData/Local/pypoetry/Cache/virtualenvs/rag-study-3qpe8pkJ-py3.11/Scripts/python.exe c:/Users/hyuk/Projects/llm-RAG-LangChain-MCP-Study/test_anthropic.py
ì•ˆë…•í•˜ì„¸ìš”! GitHub Pagesë¡œ ê°œì¸ ë¸”ë¡œê·¸ë‚˜ í¬íŠ¸í´ë¦¬ì˜¤ ì‚¬ì´íŠ¸ë¥¼ ìš´ì˜í•˜ê³  ê³„ì‹œëŠ”êµ°ìš”. GitHub PagesëŠ” ê°œë°œìë“¤ ì‚¬ì´ì—ì„œ ê°œì¸ ì›¹ì‚¬ì´íŠ¸ë¥¼ ë¬´ë£Œë¡œ í˜¸ìŠ¤íŒ…í•  ìˆ˜ ìˆëŠ” ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤. ì–´ë–¤ ë‚´ìš©ì˜ ì‚¬ì´íŠ¸ë¥¼ ìš´ì˜í•˜ê³  ê³„ì‹ ê°€ìš”? ì£¼ë¡œ ì–´ë–¤ ì£¼ì œë‚˜ ë¶„ì•¼ì— ëŒ€í•´ ë‹¤ë£¨ê³  ê³„ì‹ ì§€ ê¶ê¸ˆí•©ë‹ˆë‹¤.
ë§ì”€í•˜ì‹  ì‚¬ì´íŠ¸ ì£¼ì†ŒëŠ” https://hyukize.github.io ì…ë‹ˆë‹¤.
Message(id='msg_01QL7xjUpP1TgC6wTHDEaZVZ', content=[TextBlock(citations=None, text='ë§ì”€í•˜ì‹  ì‚¬ì´íŠ¸ ì£¼ì†ŒëŠ” https://hyukize.github.io ì…ë‹ˆë‹¤.', type='text')], model='claude-3-5-haiku-20241022', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=214, output_tokens=32, cache_creation={'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}, service_tier='standard'))
```

<br>

ëŒ€í™”ì˜ íˆìŠ¤í† ë¦¬ë¥¼ ì˜ ë°˜ì˜í•˜ê³  ìˆë‹¤.

<br>

### Streaming

```python
import anthropic
import rich
from dotenv import load_dotenv


load_dotenv()

client = anthropic.Anthropic()

prompt = "ë°°ë‹¬ìŒì‹ì„ ì‹œì¼œ ë¨¹ëŠ” ê²Œ ë‚˜ì„ê¹Œìš”? ë‚˜ê°€ì„œ ë¨¹ëŠ” ê²Œ ë‚˜ì„ê¹Œìš”??"
with client.messages.stream(
    max_tokens=1024,
    messages=[{"role": "user", "content": prompt}],
    model="claude-3-5-haiku-20241022",
) as stream:
    for event in stream:
        if event.type == "text":
            print(event.text, end="")
    print()
    
    rich.print(stream.get_final_message())
```

```result
ê°ê°ì˜ ì¥ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤:

ë°°ë‹¬ìŒì‹
ì¥ì :
- í¸ë¦¬í•¨
- ì§‘ì—ì„œ í¸ì•ˆí•˜ê²Œ ì‹ì‚¬
- ì‹œê°„ ì ˆì•½
- ë‹¤ì–‘í•œ ë©”ë‰´ ì„ íƒ ê°€ëŠ¥

ë‹¨ì :
- ë°°ë‹¬ë£Œ ì¶”ê°€ ë¹„ìš©
- ìŒì‹ í’ˆì§ˆ ì €í•˜ ê°€ëŠ¥ì„±
- ê±´ê°•ì— ì¢‹ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
- ì¼íšŒìš© ìš©ê¸° ì‚¬ìš©ìœ¼ë¡œ í™˜ê²½ ë¬¸ì œ

ì™¸ì‹
ì¥ì :
- ì‹ ì„ í•œ ìŒì‹
- ë¶„ìœ„ê¸° ì¦ê¸°ê¸°
- ì‚¬íšŒì  êµë¥˜
- ìš”ë¦¬ ê³¼ì • ê´€ì°°

ë‹¨ì :
- ë¹„ìš© ë†’ìŒ
- ì´ë™ ì‹œê°„ ì†Œìš”
- ì˜ˆì•½ í•„ìš”
- í˜¼ì¡í•  ìˆ˜ ìˆìŒ

ìƒí™©ê³¼ ê°œì¸ ì„ í˜¸ì— ë”°ë¼ ì„ íƒí•˜ì„¸ìš”.
Message(
    id='msg_01XmbEpkZPnTTZC4GxaSf4zh',
    content=[
        TextBlock(
            citations=None,
            text='ê°ê°ì˜ ì¥ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤:\n\në°°ë‹¬ìŒì‹\nì¥ì :\n- í¸ë¦¬í•¨\n- ì§‘ì—ì„œ í¸ì•ˆí•˜ê²Œ ì‹ì‚¬\n- ì‹œê°„ ì ˆì•½\n- ë‹¤ì–‘í•œ ë©”ë‰´ ì„ íƒ ê°€ëŠ¥\n\në‹¨ì :\n- ë°°ë‹¬ë£Œ ì¶”ê°€ ë¹„ìš©\n- ìŒì‹ í’ˆì§ˆ ì €í•˜ ê°€ëŠ¥ì„±\n- ê±´ê°•ì— ì¢‹ì§€ ì•Šì„ ìˆ˜ ìˆìŒ\n- ì¼íšŒìš© ìš©ê¸° ì‚¬ìš©ìœ¼ë¡œ í™˜ê²½     
ë¬¸ì œ\n\nì™¸ì‹\nì¥ì :\n- ì‹ ì„ í•œ ìŒì‹\n- ë¶„ìœ„ê¸° ì¦ê¸°ê¸°\n- ì‚¬íšŒì  êµë¥˜\n- ìš”ë¦¬ ê³¼ì • ê´€ì°°\n\në‹¨ì :\n- ë¹„ìš© ë†’ìŒ\n- ì´ë™ ì‹œê°„ ì†Œìš”\n- ì˜ˆì•½ í•„ìš”\n- í˜¼ì¡í•  ìˆ˜ ìˆìŒ\n\nìƒí™©ê³¼ ê°œì¸ ì„ í˜¸ì— ë”°ë¼ ì„ íƒí•˜ì„¸ìš”.',
            type='text'
        )
    ],
    model='claude-3-5-haiku-20241022',
    role='assistant',
    stop_reason='end_turn',
    stop_sequence=None,
    type='message',
    usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=45, output_tokens=254, cache_creation={'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}, service_tier='standard')
)
```

ì¡°ê¸ˆì”© ë‚˜ëˆ„ì–´ ì¶œë ¥ë˜ëŠ” ë°©ì‹ì„ ë³´ì•„, Streaming ì´ ì˜ êµ¬í˜„ë˜ê³  ìˆì—ˆë‹¤.

ì´ë²ˆì— ì‚¬ìš©í•˜ë©´ì„œ ëŠë‚€ ì ì´ ìˆë‹¤ë©´, Claudeì˜ í•œêµ­ì–´ê°€ ìƒë‹¹íˆ ìœ ì°½í•˜ê³  ì†ë„ê°€ ê½¤ë‚˜ ë¹ ë¥´ë‹¤.(ëª¨ë¸ ì°¨ì´ë„ ìˆê² ì§€ë§Œ)

<br>

## ë¹„ë™ê¸° ì²˜ë¦¬
